{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(f'X_train298.pkl', 'rb'))\n",
    "y_train = pickle.load(open(f'y_train298.pkl', 'rb'))\n",
    "X_test = pickle.load(open(f'X_test298.pkl', 'rb'))\n",
    "y_test = pickle.load(open(f'y_test298.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalized(X,s):\n",
    "    flattened_channels = X.reshape(3,-1)\n",
    "    channel_mean = flattened_channels.mean(dim = 1)\n",
    "    channel_stddev = flattened_channels.std(dim = 1)\n",
    "    preprocess2 = transforms.Compose([\n",
    "                      transforms.Normalize(channel_mean, channel_stddev)\n",
    "    ])\n",
    "\n",
    "\n",
    "    temptwo = torch.tensor([])\n",
    "    for i in range(X.shape[0]):\n",
    "        a = preprocess2(X[i])\n",
    "        temptwo = torch.cat([temptwo, a.reshape(1,3,s,s)])\n",
    "  \n",
    "    return temptwo\n",
    "\n",
    "\n",
    "def imageSetResize(newSize,X):\n",
    "    preprocess1 = transforms.Compose([\n",
    "                        transforms.ToPILImage(),\n",
    "                        transforms.Resize(newSize),\n",
    "                        transforms.ToTensor()])\n",
    "  \n",
    "    temp = torch.tensor([])\n",
    "    for i in range(X.shape[0]):\n",
    "        a = preprocess1(X[i])\n",
    "        temp = torch.cat([temp, a.reshape(1,3,newSize,newSize)])\n",
    "\n",
    "    return temp \n",
    "\n",
    "\n",
    "def splitTrainTest(X,y):\n",
    "    shuffled_indices = torch.randperm(X.shape[0])\n",
    "    ul = math.floor(0.8*X.shape[0])\n",
    "    train_indices = shuffled_indices[:ul]\n",
    "    test_indices = shuffled_indices[ul:]\n",
    "    # train_indices.shape[0] + test_indices.shape[0]\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]  \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    print('y_train -> [0]:{} [1]:{}'.format((y_train == 0).sum().item(), (y_train == 1).sum().item()))\n",
    "    print('y_test -> [0]:{} [1]:{}'.format((y_test == 0).sum().item(), (y_test == 1).sum().item()))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def labelize(p):\n",
    "    labelized_preds = []\n",
    "    for i in p:\n",
    "        l = 0. if i[0]>i[1] else 1.\n",
    "        labelized_preds.append(l)\n",
    "\n",
    "    return torch.tensor(labelized_preds)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_and_batch(X,y,num,bs):\n",
    "    shuffled_indices = torch.randperm(X.shape[0])\n",
    "    newX = X[shuffled_indices]\n",
    "    newY = y[shuffled_indices]\n",
    "\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(num):\n",
    "        X_batches.append(X[i*bs:(i+1)*bs])\n",
    "        y_batches.append(y_train[i*bs:(i+1)*bs])\n",
    "\n",
    "    return X_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 32\n",
    "X_train = imageSetResize(imsize, X_train.float())\n",
    "X_train = getNormalized(X_train.float(),imsize)\n",
    "X_test = imageSetResize(imsize, X_test.float())\n",
    "X_test = getNormalized(X_test.float(),imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lenetModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(lenetModel, self).__init__()\n",
    "        self.c1 = nn.Conv2d(3,6, kernel_size = 5)\n",
    "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.first_cons6_filterlist = nn.ModuleList([])\n",
    "        self.second_cons6_filterlist = nn.ModuleList([])\n",
    "        self.third_cons3_filterlist = nn.ModuleList([])\n",
    "        self.fourth_last1_filterlist = nn.ModuleList([])\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.first_cons6_filterlist.append(nn.Conv2d(3,1,kernel_size = 5))\n",
    "        for i in range(6):\n",
    "            self.second_cons6_filterlist.append(nn.Conv2d(4,1,kernel_size = 5))\n",
    "        for i in range(3):\n",
    "            self.third_cons3_filterlist.append(nn.Conv2d(4,1,kernel_size = 5))\n",
    "        self.fourth_last1_filterlist.append(nn.Conv2d(6,1,kernel_size = 5))\n",
    "        \n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size = 5)\n",
    "        self.bn3= nn.BatchNorm2d(120)\n",
    "        self.ll1 = nn.Linear(120, 84)\n",
    "        self.ll2 = nn.Linear(84,2)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        nos = x.shape[0]\n",
    "        ef = lambda x,y: torch.index_select(y,1,torch.tensor(x))\n",
    "        c1_out = self.c1(x)\n",
    "#         print(c1_out.shape)\n",
    "        out = F.relu(self.bn1(self.pool1(c1_out)))\n",
    "#         print(out.shape)\n",
    "        lione = [ef([0,1,2],out),ef([1,2,3],out), ef([2,3,4],out), ef([3,4,5],out), ef([0,4,5],out),ef([0,1,5],out)]\n",
    "        litwo = [ef([0,1,2,3],out),ef([1,2,3,4],out), ef([2,3,4,5],out), ef([0,3,4,5],out), ef([0,1,4,5],out),ef([0,1,2,5],out)]\n",
    "        lithree = [ef([0,1,3,4],out),ef([1,2,4,5],out), ef([0,2,3,5],out)]\n",
    "        lifour = [ef([0,1,2,3,4,5],out)]\n",
    "        feature_maps1 = []\n",
    "        feature_maps2 = []\n",
    "        feature_maps3 = []\n",
    "        feature_maps4 = []\n",
    "        for i in range(6):\n",
    "            feature_maps1.append(self.first_cons6_filterlist[i](lione[i]))\n",
    "        for i in range(6):\n",
    "            feature_maps2.append(self.second_cons6_filterlist[i](litwo[i]))\n",
    "        for i in range(3):\n",
    "            feature_maps3.append(self.third_cons3_filterlist[i](lithree[i]))\n",
    "        for i in range(1):\n",
    "            feature_maps4.append(self.fourth_last1_filterlist[i](lifour[i]))\n",
    "        fms = []\n",
    "        fms.extend(feature_maps1)\n",
    "        fms.extend(feature_maps2)\n",
    "        fms.extend(feature_maps3)\n",
    "        fms.extend(feature_maps4)\n",
    "#         print(fms[0].shape, fms[1].shape, fms[2].shape, fms[3].shape)\n",
    "        tfms = torch.Tensor([])\n",
    "        for i in fms:\n",
    "            tfms = torch.cat([tfms, i], dim=1)\n",
    "        c2_out = F.relu(self.bn2(self.pool2(tfms)))\n",
    "#         print(c2_out.shape)\n",
    "        c3_out = self.bn3(self.conv3(c2_out))\n",
    "#         print(c3_out.shape)\n",
    "        c3_out = c3_out.reshape(nos,120)\n",
    "        ll1_out = F.relu(self.ll1(c3_out))\n",
    "        ll2_out = self.ll2(ll1_out)\n",
    "        preds = nn.Softmax(dim=1)(ll2_out)\n",
    "#         preds = F.log_softmax(ll2_out, dim=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = lenetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './slenet.pth'\n",
    "# print('y_train -> [0]:{} [1]:{}'.format((y_train == 0).sum().item(), (y_train == 1).sum().item()))\n",
    "# print('y_test -> [0]:{} [1]:{}'.format((y_test == 0).sum().item(), (y_test == 1).sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "TrainLoss:0.6890926361083984 TrainAccuracy:0.5210084033613446  TestLoss:0.6845210790634155 TestAccuracy:0.5333333333333333\n",
      "Epoch 1\n",
      "TrainLoss:0.6813702583312988 TrainAccuracy:0.5714285714285714  TestLoss:0.6805848479270935 TestAccuracy:0.5166666666666667\n",
      "Epoch 2\n",
      "TrainLoss:0.6742944717407227 TrainAccuracy:0.6764705882352942  TestLoss:0.6772624850273132 TestAccuracy:0.55\n",
      "Epoch 3\n",
      "TrainLoss:0.6680794358253479 TrainAccuracy:0.7016806722689075  TestLoss:0.6745753884315491 TestAccuracy:0.6\n",
      "Epoch 4\n",
      "TrainLoss:0.6626802682876587 TrainAccuracy:0.7436974789915967  TestLoss:0.6724756360054016 TestAccuracy:0.65\n",
      "Epoch 5\n",
      "TrainLoss:0.6578406095504761 TrainAccuracy:0.8025210084033614  TestLoss:0.6705569624900818 TestAccuracy:0.6833333333333333\n",
      "Epoch 6\n",
      "TrainLoss:0.6528199315071106 TrainAccuracy:0.8445378151260504  TestLoss:0.6681514978408813 TestAccuracy:0.7\n",
      "Epoch 7\n",
      "TrainLoss:0.6468750834465027 TrainAccuracy:0.8571428571428571  TestLoss:0.6647977232933044 TestAccuracy:0.7\n",
      "Epoch 8\n",
      "TrainLoss:0.6403436064720154 TrainAccuracy:0.8781512605042017  TestLoss:0.6607449650764465 TestAccuracy:0.7166666666666667\n",
      "Epoch 9\n",
      "TrainLoss:0.6344714760780334 TrainAccuracy:0.8907563025210085  TestLoss:0.6572877168655396 TestAccuracy:0.7166666666666667\n",
      "Epoch 10\n",
      "TrainLoss:0.6293697357177734 TrainAccuracy:0.8907563025210085  TestLoss:0.6544609069824219 TestAccuracy:0.7166666666666667\n",
      "Epoch 11\n",
      "TrainLoss:0.6254579424858093 TrainAccuracy:0.8907563025210085  TestLoss:0.6526706218719482 TestAccuracy:0.75\n",
      "Epoch 12\n",
      "TrainLoss:0.6226020455360413 TrainAccuracy:0.907563025210084  TestLoss:0.6527563333511353 TestAccuracy:0.75\n",
      "Epoch 13\n",
      "TrainLoss:0.6205242872238159 TrainAccuracy:0.9117647058823529  TestLoss:0.6523695588111877 TestAccuracy:0.7666666666666667\n",
      "Epoch 14\n",
      "TrainLoss:0.6189237236976624 TrainAccuracy:0.9201680672268907  TestLoss:0.6510376334190369 TestAccuracy:0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(params = m1.parameters(),lr=1e-3) # Optimizer\n",
    "# tb = SummaryWriter()\n",
    "prev_testacc = -float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    X_batches, y_batches = shuffle_and_batch(X_train, y_train, 4, 67)\n",
    "    for i in range(len(X_batches)):\n",
    "        preds = m1(X_batches[i])\n",
    "#         loss = criterion(preds,labelToOneHot(y_batches[i])) \n",
    "        loss = criterion(preds,y_batches[i].long())+ 0.65*sum(p.pow(2.0).sum() for p in m1.parameters())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Checking model on training set\n",
    "    train_preds = m1(X_train)\n",
    "    train_loss = criterion(train_preds, y_train.long())\n",
    "#     train_loss = criterion(train_preds,labelToOneHot(y_train))\n",
    "    train_preds = labelize(train_preds)\n",
    "    train_prediction_comparisons = (y_train == train_preds)\n",
    "    train_accuracy = float(train_prediction_comparisons.sum())/float(y_train.shape[0])\n",
    "    print('TrainLoss:{} TrainAccuracy:{}'.format(train_loss.item(), train_accuracy), end='  ')\n",
    "    \n",
    "\n",
    "    # Checking model on testing set\n",
    "    test_preds = m1(X_test)\n",
    "    test_loss = criterion(test_preds, y_test.long())\n",
    "#     test_loss = criterion(test_preds,labelToOneHot(y_test))\n",
    "    test_preds = labelize(test_preds)\n",
    "    test_prediction_comparisons = (y_test == test_preds)\n",
    "    test_accuracy = float(test_prediction_comparisons.sum())/float(y_test.shape[0])\n",
    "    print('TestLoss:{} TestAccuracy:{}'.format(test_loss.item(), test_accuracy))\n",
    "    if test_accuracy < prev_testacc and prev_testacc>0.7:\n",
    "        break\n",
    "    state_dict = m1.state_dict()\n",
    "    torch.save(state_dict, model_path)\n",
    "    prev_testacc = test_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     tb.add_scalar('TrainLoss',train_loss, epoch)\n",
    "#     tb.add_scalar('TestLoss',test_loss, epoch)\n",
    "#     tb.add_scalar('TrainAccuracy', train_accuracy, epoch)\n",
    "#     tb.add_scalar('TestAccuracy', test_accuracy, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = lenetModel()\n",
    "n1 = torch.load(model_path)\n",
    "a1.load_state_dict(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xraw = pickle.load(open(f'X5040_32_raw.pkl', 'rb'))\n",
    "# yraw = pickle.load(open(f'y5040_32_raw.pkl', 'rb'))\n",
    "\n",
    "# raw_preds = a1(Xraw)\n",
    "# # raw_loss = criterion(raw_preds, labelToOneHot(yraw))\n",
    "# raw_preds = labelize(raw_preds)\n",
    "# raw_prediction_comparisons = (yraw == raw_preds)\n",
    "# raw_accuracy = float(raw_prediction_comparisons.sum())/float(yraw.shape[0])\n",
    "# print('RawAccuracy:{}'.format(raw_accuracy), end='  ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(yraw, raw_preds) #target on the vertical axis and preds on the horizontal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(yraw, raw_preds, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lenetModel(\n",
       "  (c1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (first_cons6_filterlist): ModuleList(\n",
       "    (0): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (2): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): Conv2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (second_cons6_filterlist): ModuleList(\n",
       "    (0): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (2): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (third_cons3_filterlist): ModuleList(\n",
       "    (0): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (2): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (fourth_last1_filterlist): ModuleList(\n",
       "    (0): Conv2d(6, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ll1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (ll2): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestLoss:0.6781192421913147 TestAccuracy:0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "test_preds = a1(X_test)\n",
    "test_loss = criterion(test_preds, y_test.long())\n",
    "#     test_loss = criterion(test_preds,labelToOneHot(y_test))\n",
    "test_preds = labelize(test_preds)\n",
    "test_prediction_comparisons = (y_test == test_preds)\n",
    "test_accuracy = float(test_prediction_comparisons.sum())/float(y_test.shape[0])\n",
    "print('TestLoss:{} TestAccuracy:{}'.format(test_loss.item(), test_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=10, out_features=2, bias=True),\n",
       " Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.ModuleList()\n",
    "\n",
    "a.append(nn.Linear(10,2))\n",
    "a.append(nn.Conv2d(3,16,kernel_size = 2))\n",
    "a[0],a[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
