{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "LeNet5.2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shouvikcirca/PCB_Defect_Detection/blob/master/LeNet5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXw93V3KBuGL",
        "colab_type": "text"
      },
      "source": [
        "# LeNet5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x42ZnnA0BuGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "\n",
        "nos = 7\n",
        "pars = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC2jsHuXB2q6",
        "colab_type": "code",
        "outputId": "dfcfaef2-d198-4066-ba13-15707c384c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoSCktJAB60c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.load('drive/My Drive/Copy of xtrain.npy')\n",
        "y = np.load('drive/My Drive/Copy of ytrain.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1zTL2ONB9Lw",
        "colab_type": "code",
        "outputId": "37bc1812-d9fe-4334-9998-d17f11e3564c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = torch.from_numpy(X)\n",
        "X = X.permute(0,3,1,2)\n",
        "y = torch.from_numpy(y)\n",
        "X.shape, y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([298, 3, 300, 300]), torch.Size([298]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLB5FrOMBuGR",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUC_1EbfBuGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNormalized(X,s):\n",
        "    flattened_channels = X.reshape(3,-1)\n",
        "    channel_mean = flattened_channels.mean(dim = 1)\n",
        "    channel_stddev = flattened_channels.std(dim = 1)\n",
        "    preprocess2 = transforms.Compose([\n",
        "                      transforms.Normalize(channel_mean, channel_stddev)\n",
        "    ])\n",
        "\n",
        "\n",
        "    temptwo = torch.tensor([])\n",
        "    for i in range(X.shape[0]):\n",
        "      a = preprocess2(X[i])\n",
        "      temptwo = torch.cat([temptwo, a.reshape(1,3,s,s)])\n",
        "  \n",
        "    return temptwo\n",
        "\n",
        "\n",
        "def imageSetResize(newSize,X):\n",
        "    preprocess1 = transforms.Compose([\n",
        "                        transforms.ToPILImage(),\n",
        "                        transforms.Resize(newSize),\n",
        "                        transforms.ToTensor()])\n",
        "  \n",
        "    temp = torch.tensor([])\n",
        "    for i in range(X.shape[0]):\n",
        "        a = preprocess1(X[i])\n",
        "        temp = torch.cat([temp, a.reshape(1,3,newSize,newSize)])\n",
        "\n",
        "    return temp \n",
        "\n",
        "\n",
        "def splitTrainTest(X,y):\n",
        "    shuffled_indices = torch.randperm(X.shape[0])\n",
        "    ul = math.floor(0.8*X.shape[0])\n",
        "    train_indices = shuffled_indices[:ul]\n",
        "    test_indices = shuffled_indices[ul:]\n",
        "    # train_indices.shape[0] + test_indices.shape[0]\n",
        "    X_train = X[train_indices]\n",
        "    y_train = y[train_indices]  \n",
        "    X_test = X[test_indices]\n",
        "    y_test = y[test_indices]\n",
        "    print('y_train -> [0]:{} [1]:{}'.format((y_train == 0).sum().item(), (y_train == 1).sum().item()))\n",
        "    print('y_test -> [0]:{} [1]:{}'.format((y_test == 0).sum().item(), (y_test == 1).sum().item()))\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def labelize(p):\n",
        "    labelized_preds = []\n",
        "    for i in p:\n",
        "        l = 0. if i[0]>i[1] else 1.\n",
        "        labelized_preds.append(l)\n",
        "\n",
        "    return torch.tensor(labelized_preds)\n",
        "\n",
        "\n",
        "\n",
        "def shuffle_and_batch(X,y,num,bs):\n",
        "    shuffled_indices = torch.randperm(X.shape[0])\n",
        "    newX = X[shuffled_indices]\n",
        "    newY = y[shuffled_indices]\n",
        "\n",
        "    X_batches = []\n",
        "    y_batches = []\n",
        "    for i in range(num):\n",
        "        X_batches.append(X[i*bs:(i+1)*bs])\n",
        "        y_batches.append(y_train[i*bs:(i+1)*bs])\n",
        "\n",
        "    return X_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj6jcCXLBuGW",
        "colab_type": "text"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQy8vua7BuGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pars = []\n",
        "bims = torch.randn(7,3,32,32)\n",
        "\n",
        "#C1\n",
        "c1 = nn.Conv2d(3,6, kernel_size = 5)\n",
        "pars = pars + list(c1.parameters())\n",
        "\n",
        "#S2\n",
        "pool1 = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "#C3\n",
        "first_cons6_filterlist = []\n",
        "second_cons6_filterlist = []\n",
        "third_cons3_filterlist = []\n",
        "fourth_last1_filterlist = []\n",
        "for i in range(6):\n",
        "    first_cons6_filterlist.append(nn.Conv2d(3,1,kernel_size = 5))\n",
        "    pars = pars + list(first_cons6_filterlist[-1].parameters())\n",
        "    \n",
        "for i in range(6):\n",
        "    second_cons6_filterlist.append(nn.Conv2d(4,1,kernel_size = 5))\n",
        "    pars = pars + list(second_cons6_filterlist[-1].parameters())\n",
        "    \n",
        "for i in range(3):\n",
        "    third_cons3_filterlist.append(nn.Conv2d(4,1,kernel_size = 5))\n",
        "    pars = pars + list(third_cons3_filterlist[-1].parameters())\n",
        "\n",
        "fourth_last1_filterlist = [nn.Conv2d(6,1,kernel_size = 5)]\n",
        "pars = pars + list(fourth_last1_filterlist[-1].parameters())\n",
        "\n",
        "#S4\n",
        "pool2 = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "#C5\n",
        "conv3 = nn.Conv2d(16, 120, kernel_size = 5)\n",
        "pars = pars + list(conv3.parameters())\n",
        "\n",
        "#F6\n",
        "ll1 = nn.Linear(120, 84)\n",
        "\n",
        "#F7\n",
        "ll2 = nn.Linear(84,2)\n",
        "pars = pars + list(ll1.parameters()) + list(ll2.parameters())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmnhG202loT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss() # Loss Function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QbXFhbBuGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayDetails(lone = None, ltwo=None):\n",
        "    if lone is not None:\n",
        "        a = lone\n",
        "        print('--------')\n",
        "        for i in a[0]:\n",
        "            print(i)\n",
        "        print('--------')\n",
        "        for i in a[1]:\n",
        "            print(i)\n",
        "        print('-------')\n",
        "        for i in a[2]:\n",
        "            print(i)\n",
        "        print('-------')\n",
        "        for i in a[3]:\n",
        "            print(i)\n",
        "    if ltwo is not None:\n",
        "        a = ltwo\n",
        "        for i in a:\n",
        "            print('-----')\n",
        "            for j in i:\n",
        "                print(j.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2xDzuPRBuGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "displayDetails(lone = [first_cons6_filterlist,second_cons6_filterlist,third_cons3_filterlist,fourth_last1_filterlist])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jMzvZ1BuGh",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC6H9PuXBuGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resized_imageset = imageSetResize(32, X.float())\n",
        "normalized_imageset = getNormalized(resized_imageset.float(),32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5d3IZAnDsfO",
        "colab_type": "code",
        "outputId": "6f4c9667-7c45-4e9f-dd43-fe5e1a7bdf3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train, y_train, X_test, y_test = splitTrainTest(resized_imageset, y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train -> [0]:119 [1]:119\n",
            "y_test -> [0]:30 [1]:30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpmIw5uaBuGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ef(li, c1_out):\n",
        "    return torch.index_select(c1_out, 1, torch.tensor(li))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdR1kcwSBuGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feedForward(X):\n",
        "    \n",
        "    global c1\n",
        "    global first_cons6_filterlist\n",
        "    global second_cons6_filterlist\n",
        "    global third_cons3_filterlist\n",
        "    global fourth_last1_filterlist\n",
        "    nos = X.shape[0]\n",
        "    c1_out = c1(X)\n",
        "    out = torch.tanh(pool1(c1_out))\n",
        "    lione = [ef([0,1,2],out),ef([1,2,3],out), ef([2,3,4],out), ef([3,4,5],out), ef([0,4,5],out),ef([0,1,5],out)]\n",
        "    litwo = [ef([0,1,2,3],out),ef([1,2,3,4],out), ef([2,3,4,5],out), ef([0,3,4,5],out), ef([0,1,4,5],out),ef([0,1,2,5],out)]\n",
        "    lithree = [ef([0,1,3,4],out),ef([1,2,4,5],out), ef([0,2,3,5],out)]\n",
        "    lifour = [ef([0,1,2,3,4,5],out)]\n",
        "    feature_maps1 = []\n",
        "    feature_maps2 = []\n",
        "    feature_maps3 = []\n",
        "    feature_maps4 = []\n",
        "    for i in range(6):\n",
        "        feature_maps1.append(first_cons6_filterlist[i](lione[i]))\n",
        "    for i in range(6):\n",
        "        feature_maps2.append(second_cons6_filterlist[i](litwo[i]))\n",
        "    for i in range(3):\n",
        "        feature_maps3.append(third_cons3_filterlist[i](lithree[i]))\n",
        "    for i in range(1):\n",
        "        feature_maps4.append(fourth_last1_filterlist[i](lifour[i]))\n",
        "    fms = []\n",
        "    fms.extend(feature_maps1)\n",
        "    fms.extend(feature_maps2)\n",
        "    fms.extend(feature_maps3)\n",
        "    fms.extend(feature_maps4)\n",
        "    tfms = torch.Tensor([])\n",
        "    for i in fms:\n",
        "        tfms = torch.cat([tfms, i], dim=1)\n",
        "    c2_out = torch.tanh(pool2(tfms))\n",
        "    c3_out = conv3(c2_out)\n",
        "    c3_out = c3_out.reshape(nos,120)\n",
        "    ll1_out = torch.tanh(ll1(c3_out))\n",
        "    ll2_out = ll2(ll1_out)\n",
        "    preds = nn.Softmax(dim=1)(ll2_out)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDMvGyz7BuGs",
        "colab_type": "code",
        "outputId": "d986b8d5-fe35-49c7-ef87-6bfe2d4de5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = optim.Adam(pars,lr=1e-3) # Optimizer\n",
        "for epoch in range(35):\n",
        "    print('Epoch {}'.format(epoch))\n",
        "    X_batches, y_batches = shuffle_and_batch(X_train, y_train, 4, 67)\n",
        "    for i in range(len(X_batches)):\n",
        "        preds = feedForward(X_batches[i])\n",
        "        loss = criterion(preds,y_batches[i].long())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph = True)\n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Checking model on training set\n",
        "    train_preds = feedForward(X_train)\n",
        "    train_loss = criterion(train_preds, y_train.long())\n",
        "    train_preds = labelize(train_preds)\n",
        "    train_prediction_comparisons = (y_train == train_preds)\n",
        "    train_accuracy = float(train_prediction_comparisons.sum())/float(y_train.shape[0])\n",
        "    print('Training Loss:{}\\tTraining Accuracy:{}'.format(train_loss.item(), train_accuracy), end='  ')\n",
        "  \n",
        "    # Checking model on testing set\n",
        "    test_preds = feedForward(X_test)\n",
        "    test_loss = criterion(test_preds, y_test.long())\n",
        "    test_preds = labelize(test_preds)\n",
        "    test_prediction_comparisons = (y_test == test_preds)\n",
        "    test_accuracy = float(test_prediction_comparisons.sum())/float(y_test.shape[0])\n",
        "    print('Testing Loss:{}\\tTesting Accuracy:{}'.format(test_loss.item(), test_accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Training Loss:0.4217287600040436\tTraining Accuracy:0.8907563025210085  Testing Loss:0.5310709476470947\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 1\n",
            "Training Loss:0.43320754170417786\tTraining Accuracy:0.8823529411764706  Testing Loss:0.5346968770027161\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 2\n",
            "Training Loss:0.419648677110672\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5293030738830566\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 3\n",
            "Training Loss:0.4147064983844757\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5293387174606323\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 4\n",
            "Training Loss:0.4196211099624634\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5309045910835266\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 5\n",
            "Training Loss:0.42672568559646606\tTraining Accuracy:0.8865546218487395  Testing Loss:0.5604240298271179\tTesting Accuracy:0.75\n",
            "Epoch 6\n",
            "Training Loss:0.4210794270038605\tTraining Accuracy:0.8907563025210085  Testing Loss:0.536567747592926\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 7\n",
            "Training Loss:0.4250239133834839\tTraining Accuracy:0.8865546218487395  Testing Loss:0.5650614500045776\tTesting Accuracy:0.75\n",
            "Epoch 8\n",
            "Training Loss:0.4225565195083618\tTraining Accuracy:0.8907563025210085  Testing Loss:0.5452865958213806\tTesting Accuracy:0.7666666666666667\n",
            "Epoch 9\n",
            "Training Loss:0.4247964024543762\tTraining Accuracy:0.8865546218487395  Testing Loss:0.5547282695770264\tTesting Accuracy:0.75\n",
            "Epoch 10\n",
            "Training Loss:0.41752296686172485\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5219616889953613\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 11\n",
            "Training Loss:0.4173080623149872\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5173417329788208\tTesting Accuracy:0.8\n",
            "Epoch 12\n",
            "Training Loss:0.41844770312309265\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5340753793716431\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 13\n",
            "Training Loss:0.41776707768440247\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5310570001602173\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 14\n",
            "Training Loss:0.41706976294517517\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5135473012924194\tTesting Accuracy:0.8\n",
            "Epoch 15\n",
            "Training Loss:0.41626301407814026\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5191698670387268\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 16\n",
            "Training Loss:0.4163726270198822\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5207851529121399\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 17\n",
            "Training Loss:0.416317880153656\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5152121186256409\tTesting Accuracy:0.8\n",
            "Epoch 18\n",
            "Training Loss:0.4161505103111267\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5184447765350342\tTesting Accuracy:0.8\n",
            "Epoch 19\n",
            "Training Loss:0.4160805344581604\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5177577137947083\tTesting Accuracy:0.8\n",
            "Epoch 20\n",
            "Training Loss:0.416077584028244\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5162869691848755\tTesting Accuracy:0.8\n",
            "Epoch 21\n",
            "Training Loss:0.41604575514793396\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5176216959953308\tTesting Accuracy:0.8\n",
            "Epoch 22\n",
            "Training Loss:0.41597452759742737\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5162014365196228\tTesting Accuracy:0.8\n",
            "Epoch 23\n",
            "Training Loss:0.415920227766037\tTraining Accuracy:0.8991596638655462  Testing Loss:0.516467809677124\tTesting Accuracy:0.8\n",
            "Epoch 24\n",
            "Training Loss:0.4158783257007599\tTraining Accuracy:0.8991596638655462  Testing Loss:0.516041100025177\tTesting Accuracy:0.8\n",
            "Epoch 25\n",
            "Training Loss:0.4158417284488678\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5157578587532043\tTesting Accuracy:0.8\n",
            "Epoch 26\n",
            "Training Loss:0.415807843208313\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5156636834144592\tTesting Accuracy:0.8\n",
            "Epoch 27\n",
            "Training Loss:0.41577985882759094\tTraining Accuracy:0.8991596638655462  Testing Loss:0.515208899974823\tTesting Accuracy:0.8\n",
            "Epoch 28\n",
            "Training Loss:0.4157796800136566\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5154399275779724\tTesting Accuracy:0.8\n",
            "Epoch 29\n",
            "Training Loss:0.41590142250061035\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5145689845085144\tTesting Accuracy:0.8\n",
            "Epoch 30\n",
            "Training Loss:0.4166792929172516\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5166001915931702\tTesting Accuracy:0.8\n",
            "Epoch 31\n",
            "Training Loss:0.4170730710029602\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5145772099494934\tTesting Accuracy:0.8\n",
            "Epoch 32\n",
            "Training Loss:0.418108195066452\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5233761072158813\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 33\n",
            "Training Loss:0.4177932143211365\tTraining Accuracy:0.8949579831932774  Testing Loss:0.5248857736587524\tTesting Accuracy:0.7833333333333333\n",
            "Epoch 34\n",
            "Training Loss:0.4157352149486542\tTraining Accuracy:0.8991596638655462  Testing Loss:0.5188366770744324\tTesting Accuracy:0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcOnFXPuN8Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJgtiIk8N8RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eODpPQEEN8Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}